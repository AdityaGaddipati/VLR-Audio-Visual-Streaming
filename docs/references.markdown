---
layout: page
title: References
permalink: /references/
---

[1] Shi, B., W.-N. Hsu, K. Lakhotia, et al. Learning audio-visual speech representation by masked219
multimodal cluster prediction, 2022.220

[2] Watanabe, S., T. Hori, S. Karita, et al. Espnet: End-to-end speech processing toolkit, 2018.221

[3] Tsunoo, E., Y. Kashiwagi, S. Watanabe. Streaming transformer asr with blockwise synchronous222
beam search, 2020.223

[4] Palaskar, S., R. Sanabria, F. Metze. End-to-end multimodal speech recognition. In 2018224
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages225
5774–5778. 2018.226

[5] Yu, W., S. Zeiler, D. Kolossa. Fusing information streams in end-to-end audio-visual speech227
recognition, 2021.228

[6] Ghazvininejad, M., O. Levy, Y. Liu, et al. Mask-predict: Parallel decoding of conditional229
masked language models. In Proceedings of the 2019 Conference on Empirical Methods in230
Natural Language Processing and the 9th International Joint Conference on Natural Language231
Processing (EMNLP-IJCNLP), pages 6112–6121. Association for Computational Linguistics,232
Hong Kong, China, 2019.233

[7] Wang, T., Y. Fujita, X. Chang, et al. Streaming end-to-end asr based on blockwise non-234
autoregressive models, 2021.235

[8] Afouras, T., J. S. Chung, A. Zisserman. Lrs3-ted: a large-scale dataset for visual speech236
recognition, 2018.